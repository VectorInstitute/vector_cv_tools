{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the environment - \"vector_cv_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pkgs/anaconda3/bin/python\n",
      "/pkgs/vector_cv_project/lib/python3.6/site-packages\n",
      "/pkgs/nccl_2.8.3-1+cuda10.1_x86_64/lib:/pkgs/cudnn-10.1-v7.6.4.38/lib64:/pkgs/cuda-10.1/lib64\n",
      "/pkgs/vector_cv_project/lib/python3.6/site-packages/bin:/pkgs/cuda-10.1/bin:/pkgs/anaconda3/bin:/pkgs/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/h/xinli/.local/bin\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!echo $PYTHONPATH\n",
    "!echo $LD_LIBRARY_PATH\n",
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from vector_cv_tools import datasets\n",
    "from vector_cv_tools import transforms as T\n",
    "\n",
    "from vector_cv_tools import utils\n",
    "\n",
    "import albumentations\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics_annotation_path = \"./datasets/kinetics/kinetics700/train.json\"\n",
    "kinetics_data_path = \"./datasets/kinetics/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic, un-transformed kinetics dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define basic spatial and temporal transforms\n",
    "base_spatial_transforms = T.ComposeVideoSpatialTransform([albumentations.ToFloat(max_value=255)])\n",
    "base_temporal_transforms = T.ComposeVideoTemporalTransform([T.video_transforms.ToTensor()])\n",
    "\n",
    "# create raw dataset\n",
    "data_raw = datasets.KineticsDataset(\n",
    "        fps=10,\n",
    "        max_frames=128,\n",
    "        round_source_fps=False,\n",
    "        annotation_path = kinetics_annotation_path,\n",
    "        data_path = kinetics_data_path,\n",
    "        class_filter = [\"push_up\", \"pull_ups\"],\n",
    "        spatial_transforms=base_spatial_transforms,\n",
    "        temporal_transforms=base_temporal_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through the dataset, 2 labels, 1893 data points in total\n",
      "push_up                                  ID: 0 size: 964 ||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "pull_ups                                 ID: 1 size: 929 ||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    }
   ],
   "source": [
    "labels = data_raw.metadata.labels\n",
    "print(\"Looping through the dataset, {} labels, {} data points in total\".\n",
    "        format(data_raw.num_classes, len(data_raw)))\n",
    "for label, info in labels.items():\n",
    "    print(\"{:<40} ID: {} size: {} {}\".\n",
    "        format(label, info[\"id\"], len(info[\"indexes\"]), len(info[\"indexes\"])//20 * \"|\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 480, 272, 3])\n",
      "{'label_ids': [0], 'label_names': ['push_up'], 'sampled_fps': 10}\n"
     ]
    }
   ],
   "source": [
    "data_point, label = data_raw[0]\n",
    "print(data_point.shape)\n",
    "print(label)\n",
    "vid = (data_point.numpy() * 255).astype(np.uint8)\n",
    "utils.create_GIF(\"raw_img.gif\", vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dataset with video transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial transforms: \n",
      "ComposeVideoSpatialTransform(\n",
      "    ToFloat(always_apply=False, p=1.0, max_value=255)\n",
      ")\n",
      "Temporal transforms: \n",
      "ComposeVideoTemporalTransform(\n",
      "    TorchvisionWrapper for ColorJitter(brightness=None, contrast=None, saturation=None, hue=None)\n",
      "    TorchvisionWrapper for <function hflip at 0x7f7fa28611e0>\n",
      "    AlbumentationWrapper for VerticalFlip(always_apply=False, p=1)\n",
      "    RandomResizedSpatialCrop(size=(280, 280), scale=(0, 1), ratio=(0.75, 1.3333), interpolation=PIL.Image.BILINEAR)\n",
      "    RandomTemporalCrop(size=50, padding=None)\n",
      "    SampleEveryNthFrame(n=2)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "###############################################\n",
    "##### NOW PRESENT TO YOU: VideoTransforms!!####\n",
    "###############################################\n",
    "\n",
    "# compatibility with others\n",
    "transform1 = T.from_torchvision(\n",
    "    torchvision.transforms.ColorJitter())\n",
    "\n",
    "transform2 = T.from_torchvision(\n",
    "    torchvision.transforms.functional.hflip)\n",
    "\n",
    "transform3 = T.from_albumentation(\n",
    "    albumentations.VerticalFlip(p=1))\n",
    "\n",
    "# Spatial: in-house\n",
    "transform4 = T.RandomResizedSpatialCrop((280, 280), scale=(0, 1))\n",
    "transform5 = T.RandomSpatialCrop((480, 480))\n",
    "transform6 = T.RandomTemporalCrop(size=50, pad_if_needed=True, padding_mode=\"wrap\")\n",
    "transform7 = T.SampleEveryNthFrame(2)\n",
    "transform8 = T.ToTensor()\n",
    "                                  \n",
    "spatial_transforms = base_spatial_transforms\n",
    "\n",
    "# define temporal transforms\n",
    "temporal_transforms = [transform1, transform2, transform3, transform4,\n",
    "                        transform6, transform7, transform8]\n",
    "\n",
    "temporal_transforms = T.ComposeVideoTemporalTransform(temporal_transforms)\n",
    "\n",
    "print(\"Spatial transforms: \\n{}\".format(spatial_transforms))\n",
    "print(\"Temporal transforms: \\n{}\".format(temporal_transforms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset with transformations\n",
    "data_transformed = datasets.KineticsDataset(\n",
    "        fps=10,\n",
    "        max_frames=128,\n",
    "        round_source_fps=False,\n",
    "        annotation_path = kinetics_annotation_path,\n",
    "        data_path = kinetics_data_path,\n",
    "        class_filter = [\"push_up\", \"pull_ups\"],\n",
    "        spatial_transforms=spatial_transforms,\n",
    "        temporal_transforms=temporal_transforms,)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 280, 280, 3])\n",
      "{'label_ids': [0], 'label_names': ['push_up'], 'sampled_fps': 10}\n"
     ]
    }
   ],
   "source": [
    "data_point, label = data_transformed[0]\n",
    "print(data_point.shape)\n",
    "print(label)\n",
    "vid = (data_point.numpy() * 255).astype(np.uint8)\n",
    "utils.create_GIF(\"transformed_img.gif\", vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
