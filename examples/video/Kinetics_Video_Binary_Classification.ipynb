{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from vector_cv_tools import datasets\n",
    "from vector_cv_tools import transforms as T\n",
    "from vector_cv_tools import utils\n",
    "from vector_cv_tools import checkpointing\n",
    "\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kinetics_annotation_path = \"./datasets/kinetics/kinetics700/train.json\"\n",
    "kinetics_data_path = \"./datasets/kinetics/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "LOG_FILE = \"run_log.out\"\n",
    "\n",
    "def log_and_print(print_str):\n",
    "    logging.info(print_str)\n",
    "    print(print_str)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    filename=LOG_FILE,\n",
    "                    filemode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define spatial and temporal transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial transforms: \n",
      "ComposeVideoSpatialTransform(\n",
      "    Resize(always_apply=False, p=1, height=128, width=128, interpolation=1)\n",
      "    ToFloat(always_apply=False, p=1.0, max_value=255)\n",
      ")\n",
      "Temporal transforms: \n",
      "ComposeVideoTemporalTransform(\n",
      "    RandomTemporalCrop(size=64, padding=None)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define spatial transforms\n",
    "spatial_transforms = [A.Resize(128, 128), A.ToFloat(max_value=255)]\n",
    "spatial_transforms = T.ComposeVideoSpatialTransform(spatial_transforms)\n",
    "\n",
    "# define temporal transforms\n",
    "temporal_transforms = [ T.video_transforms.RandomTemporalCrop(size=64, \n",
    "                                                    pad_if_needed=True,\n",
    "                                                    padding_mode =\"wrap\"), \n",
    "                        T.video_transforms.ToTensor()]\n",
    "\n",
    "temporal_transforms = T.ComposeVideoTemporalTransform(temporal_transforms)\n",
    "\n",
    "print(\"Spatial transforms: \\n{}\".format(spatial_transforms))\n",
    "print(\"Temporal transforms: \\n{}\".format(temporal_transforms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset given the annotation files and data files for Kinetics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "push_up                                  ID: 0 size: 964 ||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "pull_ups                                 ID: 1 size: 929 ||||||||||||||||||||||||||||||||||||||||||||||\n"
     ]
    }
   ],
   "source": [
    "# create dataset, only filter two classes here\n",
    "dataset = datasets.KineticsDataset(\n",
    "        fps=10,\n",
    "        max_frames=128,\n",
    "        round_source_fps=False,\n",
    "        annotation_path = kinetics_annotation_path,\n",
    "        data_path = kinetics_data_path,\n",
    "        class_filter = [\"push_up\", \"pull_ups\"],\n",
    "        spatial_transforms=spatial_transforms,\n",
    "        temporal_transforms=temporal_transforms,)\n",
    "\n",
    "# inspect labels\n",
    "labels = dataset.metadata.labels\n",
    "\n",
    "for label, info in labels.items():\n",
    "    print(\"{:<40} ID: {} size: {} {}\".\n",
    "        format(label, info[\"id\"], len(info[\"indexes\"]), len(info[\"indexes\"])//20 * \"|\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Savable Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping through the dataset, 2 labels, 237 data points in total\n"
     ]
    }
   ],
   "source": [
    "# convert data to loader\n",
    "num_workers = 4\n",
    "batch_size = 8\n",
    "###################### CHECKPOINTING ########################\n",
    "# The dataloader need to keep state since we need to checkpoint within an epoch \n",
    "loader = checkpointing.SaveableDataLoader(\n",
    "                dataset,\n",
    "                num_workers=num_workers,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=utils.VideoCollateFnSelector(\"stack_or_combine\"),\n",
    "                shuffle=True)\n",
    "###########################################################\n",
    "\n",
    "\n",
    "print(\"Looping through the dataset, {} labels, {} data points in total\".\n",
    "        format(dataset.num_classes, len(loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize videos from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 128, 128, 3])\n",
      "{'label_ids': [0], 'label_names': ['push_up'], 'sampled_fps': 10}\n"
     ]
    }
   ],
   "source": [
    "data_point, label = dataset[0]\n",
    "print(data_point.shape)\n",
    "print(label)\n",
    "vid = (data_point.numpy() * 255).astype(np.uint8)\n",
    "utils.create_GIF(\"TestImage.gif\", vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a pre-trained model and change the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VideoResNet(\n",
      "  (stem): BasicStem(\n",
      "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
      "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_classes = dataset.num_classes\n",
    "\n",
    "model = torchvision.models.video.r3d_18(pretrained=True, progress=True, num_classes=400)\n",
    "# freeze the layers except for the last one\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "set_parameter_requires_grad(model, feature_extracting=True)\n",
    "\n",
    "model.fc = torch.nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### CHECKPOINTING ########################\n",
    "\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.002)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "cur_epoch = checkpointing.SavebleNumber(0)\n",
    "cur_iter = checkpointing.SavebleNumber(0)\n",
    "\n",
    "losses = checkpointing.SaveableList()\n",
    "acc = checkpointing.SaveableList()\n",
    "rng = checkpointing.SaveableRNG(888)\n",
    "\n",
    "checkpoint = checkpointing.SavableCollection(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loader=loader,\n",
    "        cur_epoch=cur_epoch,\n",
    "        cur_iter=cur_iter,\n",
    "        losses=losses,\n",
    "        acc=acc,\n",
    "        rng=rng,\n",
    "        )\n",
    "\n",
    "manager = checkpointing.CheckpointManager(checkpoint=checkpoint, \n",
    "                              directory=\"./kinetics _train_checkpoints\", \n",
    "                              max_to_keep=3,\n",
    "                              checkpoint_interval=120, # 120s\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize or restore from checkpoint\n",
    "**Note:\n",
    "`load_latest_checkpoint()` does nothing if there is no checkpoint loaded otherwise it loads from the latest checkpoint in the directory specified above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint that has finished epoch 0, iteration 221 with losses: [] is loaded from kinetics _train_checkpoints/checkpoint.12.pt\n"
     ]
    }
   ],
   "source": [
    "manager.load_latest_checkpoint()\n",
    "loaded = manager.latest_checkpoint is not None\n",
    "if loaded:\n",
    "    print_str = (f\"Checkpoint that has finished epoch {checkpoint.cur_epoch}, iteration {checkpoint.cur_iter} with \"\n",
    "                 f\"losses: {checkpoint.losses} is loaded from {manager.latest_checkpoint}\")\n",
    "\n",
    "else:\n",
    "    print_str = f\"No checkpoints found under {manager.directory}, starting from scratch\"\n",
    "\n",
    "log_and_print(print_str)\n",
    "########################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "Iteration 221, Loss: 0.5461950898170471\n",
      "Iteration 222, Loss: 0.7653592824935913\n",
      "Iteration 223, Loss: 0.5180494785308838\n",
      "Iteration 224, Loss: 0.4324283301830292\n",
      "Iteration 225, Loss: 0.524896502494812\n",
      "Iteration 226, Loss: 0.6371507048606873\n",
      "Iteration 227, Loss: 0.5810372233390808\n",
      "Iteration 228, Loss: 0.42662107944488525\n",
      "Iteration 229, Loss: 0.46102991700172424\n",
      "Iteration 230, Loss: 0.4658900797367096\n",
      "Iteration 231, Loss: 0.6230384111404419\n",
      "Iteration 232, Loss: 0.539046049118042\n",
      "Iteration 233, Loss: 0.43059056997299194\n",
      "Iteration 234, Loss: 0.5698506236076355\n",
      "Iteration 235, Loss: 0.2969943881034851\n",
      "Iteration 236, Loss: 0.5753830075263977\n",
      "Epoch took      89.87s\n",
      "Average time per batch 0.37920015471897045\n",
      "Accuracy: 0.7520000338554382\n",
      "Epoch Loss: 0.03541586809017487\n",
      "Epoch 1/49\n",
      "----------\n",
      "Iteration 0, Loss: 0.2884337306022644\n",
      "Iteration 1, Loss: 0.937825620174408\n",
      "Iteration 2, Loss: 0.42413580417633057\n",
      "Iteration 3, Loss: 0.25568124651908875\n",
      "Iteration 4, Loss: 0.43189582228660583\n",
      "Iteration 5, Loss: 0.8056589365005493\n",
      "Iteration 6, Loss: 0.6821948289871216\n",
      "Iteration 7, Loss: 0.3446059226989746\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "num_epochs = 50\n",
    "ites_per_epoch = len(loader)\n",
    "\n",
    "while cur_epoch < num_epochs:\n",
    "    print('Epoch {}/{}'.format(cur_epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    start = time.time()\n",
    "    total = running_corrects = 0\n",
    "    total_loss = 0\n",
    "###################### CHECKPOINTING ########################\n",
    "    # the loader does not know at which iteration it will start \n",
    "    # when it is loaded again from a checkpoint. If we simply\n",
    "    # enumerate, the loader will go through the rest of the datapoints, \n",
    "    # but the counting of \"idx\" will be wrong\n",
    "    # Therefore, we should not reset the value of cur_iter when\n",
    "    # it is freshly loaded from the checkpoint\n",
    "    if not loaded:\n",
    "        cur_iter.set_val(0)\n",
    "    loaded = False\n",
    "    for idx, (d, l) in enumerate(loader):\n",
    "############################################################\n",
    "\n",
    "        ########### Tweak input ##########\n",
    "        # depending on what your model wants, tensor shapes may require a permute\n",
    "        inputs = d.to(device).permute(0, 4, 2, 3, 1)\n",
    "        \n",
    "        # for single class, we just use the 0th element in the label\n",
    "        labels = [li[\"label_ids\"][0] for li in l]\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "        total += len(labels)\n",
    "        \n",
    "        log_and_print(\"Iteration {}, Loss: {}\".format(cur_iter, loss.item()))\n",
    "        \n",
    "###################### CHECKPOINTING ########################\\\n",
    "        cur_iter.add_(1)\n",
    "        manager.save(do_logging=True)\n",
    "#############################################################\n",
    "    \n",
    "    duration = time.time() - start\n",
    "    accuracy = running_corrects / total\n",
    "    loss =  total_loss / len(loader)\n",
    "    print_str = \"\\n\".join([\n",
    "            \"Epoch took {:10.2f}s\".format(duration),\n",
    "            \"Average time per batch {}\".format(duration/ites_per_epoch),\n",
    "            \"Accuracy: {}\".format(accuracy),\n",
    "            \"Epoch Loss: {}\".format(loss)\n",
    "    ])\n",
    "    log_and_print(print_str)\n",
    "###################### CHECKPOINTING ########################\\\n",
    "    losses.append(loss)\n",
    "    acc.append(accuracy)\n",
    "    cur_epoch.add_(1)\n",
    "###################### CHECKPOINTING ########################\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
